{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b026b3-e6a7-452a-a173-8f35b720fe99",
   "metadata": {},
   "source": [
    "# WhisperX template\n",
    "\n",
    "## Step 1: Change kernel\n",
    "Make sure in the top right of this window the selected kernel is `whisperX (ipykernel)`. If you see `Python 3 (ipykernel)`, just click on it to change it to the WhisperX kernel.\n",
    "\n",
    "## Step 2: Create input and output folders\n",
    "It is recommended to store audio files in a (new) folder on a storage volume. The storage volume can be found here: `~/data/volume_2`. \n",
    "- Navigate to the storage volume in the panel on the left.\n",
    "- Create a new folder called `my_transcription_project` by clicking on the `New Folder` button (folder with a plus sign) in the panel on the left.\n",
    "    - _if you choose a different folder name, remember to change the foldernames in the code cells below_\n",
    "- Go into this folder by clicking on it in the panel on the left.\n",
    "- Create a folder called `data`\n",
    "- Create another folder called `output` \n",
    "\n",
    "## Step 3: Upload audio\n",
    "- To upload an audio file, navigate to `data` folder (previous step) \n",
    "- Press the 'Upload Files' button (upward arrow) in the panel on the left to upload the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff8416-0041-48ff-b81c-3cd83a1ca197",
   "metadata": {},
   "source": [
    "## Step 4: Run the code cell below to import whisperX and define required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce39751-1509-44d7-8545-3acc169fefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import whisperx\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" \n",
    "batch_size = 16                 # reduce if low on GPU mem\n",
    "compute_type = \"float16\"        # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "whisperx_model = \"large-v2\"     # options: \"base\", \"small\", \"medium\"\n",
    "\n",
    "writer_options = {\"max_line_width\":None,\n",
    "                  \"max_line_count\":None,\n",
    "                  \"highlight_words\":None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42580b",
   "metadata": {},
   "source": [
    "## Step 5: Specify audio file\n",
    "\n",
    "In the cell below, change `audio.mp3` to the name of your audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"audio.mp3\" # change audio.mp3 to the file name of your file\n",
    "audio_file = \"/data/volume_2/my_transcription_project/data/\" + filename # change the path when relevant (e.g. when the you chose a different folder name for this project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e27c09-c5f8-4cad-bfd4-ce64f187b494",
   "metadata": {},
   "source": [
    "## Step 6: Transcribe audio file\n",
    "\n",
    "Run the code cell below to transcribe the audio file with the model selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_whisperx = whisperx.load_audio(audio_file)\n",
    "model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "result = model.transcribe(audio_whisperx, batch_size=batch_size)\n",
    "print(result[\"segments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efc7ab-ce31-4181-bc7f-c03fd966d03e",
   "metadata": {},
   "source": [
    "## Step 7:  Save output to files\n",
    "Run the cell below to save the transcript in all file formats that are supported by Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15a82f-176e-47f9-9c12-a5f178086773",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"/data/volume_2/my_transcription_project/output/\"\n",
    "writer = whisper.utils.get_writer(\"all\", output_directory)\n",
    "writer(result, audio_file, writer_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a54c0-61d4-4f5b-a114-b2b8b1f23635",
   "metadata": {},
   "source": [
    "## Additional tasks: Translate\n",
    "Run the code cell below to get a translated transcript in English from audio in a different language using the model selected above. Other target languages are currently not supported. Use step 7 to save the translated transcript in all file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "result = model.transcribe(audio_whisperx, batch_size=batch_size, task=\"translate\")\n",
    "print(result[\"segments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0d898-3b02-4a9e-89ed-9f52dca5f39f",
   "metadata": {},
   "source": [
    "## Additional Tasks: Align whisper output\n",
    "\n",
    "Improved alignment of the transcript and word level timings can be obtained by running the code cell below. Use step 7 to save the translated transcript in all file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef55e3-52ba-4bc9-a022-be5a2cda8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio_whisperx, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae83c67-1b0e-4ddf-b2a1-8da51e8af756",
   "metadata": {},
   "source": [
    "## Additional Tasks: Speaker diarization\n",
    "\n",
    "To be able to use this pipeline, you will need a Huggingface token and accept the terms for the relevant models. \n",
    "\n",
    "Step 1: create a token [here](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Step 2: Enter your token in the code cell below and run it. If you haven't accepted the terms earlier, you will get an error message with a link to accept the terms for the relevant model (Segmentation , Voice Activity Detection (VAD), and Speaker Diarization), follow the link and accept the terms and rerun the code cell below. You should now get another error with a new link. Repeat the process until you have accepted the terms for all models actually get output instead of an error message.  \n",
    "\n",
    "Use step 7 to save the translated transcript in all file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efe0df-1e44-4e46-8130-6b702b956e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_HF_TOKEN = '<insert your huggingface token here>'\n",
    "\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio_whisperx)\n",
    "# diarize_model(audio_file, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917876a6",
   "metadata": {},
   "source": [
    "## GPU Memory issues\n",
    "\n",
    "Dependening on the length of the audio file, the model used, and the GPU device used, you may run into memory issues. If this happens, you can delete the model and re-load it with a different `compute-type`. You can also try a smaller batch size, see step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e02403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete model if low on GPU resources\n",
    "import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
