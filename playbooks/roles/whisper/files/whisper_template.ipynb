{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b026b3-e6a7-452a-a173-8f35b720fe99",
   "metadata": {},
   "source": [
    "# Whisper template\n",
    "\n",
    "The code cells below can be used to import whisper and transcribe and translate audio files. The code assumes the filename of the audio file is `audio.mp3`. Change the the variable `audio` below if your filename is different.\n",
    "\n",
    "To upload an audio file, first navigate to the folder where you want to upload the audio in the panel on the left. It is recommended to store the files in a (new) folder in `/data/volume_2`. Press the 'Upload Files' button (upward arrow) in the panel on the left to upload the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff8416-0041-48ff-b81c-3cd83a1ca197",
   "metadata": {},
   "source": [
    "### Import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce39751-1509-44d7-8545-3acc169fefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b97ff1-bca0-4ec8-8cb8-80fbe73a7849",
   "metadata": {},
   "source": [
    "### Load model and specify audio file\n",
    "Run the code cell below to load the Whisper model that you need. To select a different model, change 'medium' to any of the following options:\n",
    "\n",
    "**Model options:**  \n",
    "- `'tiny'`\n",
    "- `'base'`\n",
    "- `'small'`\n",
    "- `'medium'`\n",
    "- `'large'`  \n",
    "**English only models:**  \n",
    "- `'tiny.en'`\n",
    "- `'base.en'`\n",
    "- `'small.en'`\n",
    "- `'medium.en'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faddaffb-54e6-4582-8ec8-1d5a2b6c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model('medium')     \n",
    "audio = \"/data/volume_2/audiofiles/audio.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e27c09-c5f8-4cad-bfd4-ce64f187b494",
   "metadata": {},
   "source": [
    "### Transcribe\n",
    "Run the code cell below to transcribe the audio file with the model selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250f9ec-9008-4623-92e7-259434a008b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a54c0-61d4-4f5b-a114-b2b8b1f23635",
   "metadata": {},
   "source": [
    "### Translate\n",
    "Run the code cell below to get a translated transcript in English from audio in a different language using the model selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf630017-2f34-48c1-8560-70357762b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio, task='translate', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19543204-38b6-406a-bdd8-fbece798911f",
   "metadata": {},
   "source": [
    "It is possible to specify the input language, this may improve accuracy or efficiency in some cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3947fb-0b40-4c80-ba7a-7d6c6a75b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio, task='translate', language='nl', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efc7ab-ce31-4181-bc7f-c03fd966d03e",
   "metadata": {},
   "source": [
    "### Save output to files\n",
    "Run this cell to save the transcript in all file formats that are supported by Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15a82f-176e-47f9-9c12-a5f178086773",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"./\"\n",
    "options = {\"max_line_width\":None,\n",
    "           \"max_line_count\":None,\n",
    "           \"highlight_words\":None}\n",
    "\n",
    "writer = whisper.utils.get_writer(\"all\", output_directory)\n",
    "writer(result, audio, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d49c80-c242-43bf-8189-c16c6eb71e12",
   "metadata": {},
   "source": [
    "To just print the plain text to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a307-5a9a-4c13-9791-97c1d52b94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"audio_sample.txt\", \"w+\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7884e09-5dbf-42f7-853f-72b7eddd6635",
   "metadata": {},
   "source": [
    "# WhisperX\n",
    "\n",
    "[WhisperX](https://github.com/m-bain/whisperX) can be used to improve the accuracy of timestamps, get word level timestamps, speaker diarization, and more.\n",
    "\n",
    "To enable speaker diarization, include your Hugging Face access token in the cell below. A token can be generated from [Here](https://huggingface.co/settings/tokens). On the huggingface website you need to accept the user agreement for the following models: Segmentation , Voice Activity Detection (VAD), and Speaker Diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee2289-ac8a-4098-8350-d22a03e00949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc \n",
    "\n",
    "device = \"cuda\" \n",
    "audio_file = \"audio.mp3\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c5c8c-216b-409e-9f7c-2962eadc6df9",
   "metadata": {},
   "source": [
    "### 1. Transcribe with original whisper (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ad1ce-22c9-4595-b4b9-845f2b9b9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisperx.load_model(\"small\", device, compute_type=compute_type)\n",
    "\n",
    "audio_whisperx = whisperx.load_audio(audio)\n",
    "result = model.transcribe(audio_whisperx, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0d898-3b02-4a9e-89ed-9f52dca5f39f",
   "metadata": {},
   "source": [
    "### 2. Align whisper output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef55e3-52ba-4bc9-a022-be5a2cda8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio_whisperx, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae83c67-1b0e-4ddf-b2a1-8da51e8af756",
   "metadata": {},
   "source": [
    "### 3. Assign speaker labels\n",
    "\n",
    "If you don't have a Huggingface token, create one [here](https://huggingface.co/settings/tokens)\n",
    "make sure you accept the user agreement for the following models: Segmentation , Voice Activity Detection (VAD), and Speaker Diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efe0df-1e44-4e46-8130-6b702b956e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_HF_TOKEN = '<insert your huggingface token here>'\n",
    "\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio_file, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c88fa1-8ce8-4188-8a81-a7fa18db95a4",
   "metadata": {},
   "source": [
    "### 4. Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bb642-2d86-4b8d-b5fe-90c803abad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(result[\"segments\"], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
